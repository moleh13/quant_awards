10-Week Coding Roadmap: Graph-Guided RL for Adaptive Crypto Portfolio Optimization

Week 1: Project Setup and Code Structure Planning
	•	Repository & Environment Setup: Initialize a Git repository and set up a Python environment with required libraries (e.g., stable-baselines3, torch, networkx, torch_geometric, hmmlearn, pykalman). Create a clear project structure with folders like data_pipeline/, environment/, agents/, models/, utils/, and tests/.
	•	Configuration Module: Implement a configuration file or module (e.g. config.py) to centralize key settings (asset list, date range, initial capital, transaction cost rate, etc.) for easy reuse and reproducibility.
	•	Code Skeletons: Create placeholder Python scripts for major components with modular design. For example, stub out data_pipeline/data_loader.py, environment/portfolio_env.py, agents/train_agent.py, etc., with class definitions and TODO comments. Ensure these modules can be imported without errors.
	•	Dependency Planning: Outline module interactions and sequencing (e.g., data loader provides data to environment, environment used by agents). Document these plans in the code (comments) to guide implementation in subsequent weeks. This scaffolding in Week 1 sets up a clean, maintainable codebase for the project.

Week 2: Data Pipeline Implementation
	•	Data Ingestion: Implement data_pipeline/data_loader.py to fetch historical price data for the chosen cryptocurrencies. Use an API or local CSV files to retrieve OHLCV data. Include functions to handle data for multiple assets and merge into a unified format (e.g., a pandas DataFrame with a datetime index and asset price columns).
	•	Preprocessing & Feature Engineering: In data_pipeline/preprocessor.py, implement functions to compute necessary features from raw prices. Examples: price relative returns (ratio of current price to previous price) for each asset, technical indicators (moving averages, volatility), etc. Normalize or scale features as needed (without future data leakage).
	•	Train-Test Split: If doing offline backtesting, split the data into training, validation, and test periods. Implement code to segment the historical data accordingly, ensuring the environment can access each segment separately (for training vs. evaluation).
	•	Testing Data Module: Write a simple test script (e.g., tests/test_data_pipeline.py) to load a small subset of data and verify shapes and correctness (e.g., no NaNs after preprocessing, reasonable feature values). This week’s output is a robust data loading module feeding consistent, cleaned data into the RL environment in later weeks.

Week 3: Custom Portfolio Environment Development
	•	Gym Environment Class: Implement the CryptoPortfolioEnv class in environment/portfolio_env.py inheriting from gym.Env. Define the observation space (e.g., a vector of features for each asset such as recent returns, prices, and possibly cash position) and action space (continuous vector representing portfolio weights for each asset, including a cash or “no investment” asset). Ensure the action space is constrained properly (e.g., weights sum to 1 by normalization inside the environment).
	•	Reset and Initialization: Implement CryptoPortfolioEnv.reset(). This should initialize the portfolio state at the start of an episode: set initial portfolio value (from config, e.g. $10000), start with all cash or a predefined allocation, and reset the time index to the beginning of the training data. Return the initial observation (features at the first timestep).
	•	Step Function Mechanics: Implement CryptoPortfolioEnv.step(action). This function will:
	•	Take the action (allocation weights) and apply it to the current portfolio value to determine holdings of each asset (simulate rebalancing).
	•	Advance the environment’s time index by one step and retrieve the new prices for assets.
	•	Calculate portfolio new value: e.g., sum of (asset holdings * new prices). Include a cash asset that accrues no profit (or a risk-free rate if modeling cash interest).
	•	Compute reward as the portfolio return (e.g., change in portfolio value, or log return) for that step. This reward drives the RL agent’s optimization.
	•	Apply transaction costs if applicable (subtract a small fee based on turnover – implement this if specified in config, possibly as a percentage fee per trade).
	•	Update the state for the next step (e.g., update holdings or weights, compute new observation features for time t+1). Mark done=True if at end of dataset/episode.
	•	Return (observation, reward, done, info) as per Gym API.
	•	Validation of Environment: Using sample data from Week 2, test the environment by running a few steps with random actions. Verify that portfolio value updates correctly and the observation reflects the next timestep’s data. This custom environment ensures we are not using any FinRL or TradeMaster pre-made env, but building our own trading simulator ￼ ￼, fulfilling the requirement of coding from scratch.

Week 4: Baseline RL Agent and Training Pipeline
	•	Stable-Baselines3 Integration: Set up a training script agents/train_baseline.py to train an RL agent on the environment. Use Stable-Baselines3 with a suitable algorithm (Proximal Policy Optimization (PPO) or Soft Actor-Critic (SAC)) to handle the continuous action space ￼. Configure the agent with a straightforward MLP policy network (as a baseline without graph features yet) and hyperparameters (learning rate, discount factor, etc.) defined in the config.
	•	Training Loop: In the script, instantiate CryptoPortfolioEnv for the training data. Use SB3’s .learn() method to train the agent for a certain number of timesteps or episodes. Enable logging of training progress (episode rewards, etc.) using SB3’s Monitor or callback mechanisms. Ensure that the environment’s seed method is called for reproducibility.
	•	Model Saving: After training, save the trained model to disk (using SB3’s model.save() function). Also save any normalization statistics from the environment if used. This allows reloading the agent later for evaluation.
	•	Baseline Evaluation Script: Implement agents/evaluate_baseline.py to test the trained baseline agent. This script will load the saved model, create a new CryptoPortfolioEnv for the test dataset, and run the agent in a loop through the test episode (no training, just action inference). Record performance metrics such as total return.
	•	Quick Performance Check: Calculate basic metrics like cumulative return or average reward from the evaluation run (you will expand metrics in Week 5). If the agent performs poorly (e.g., worse than random or equal-weight), note that this is expected for an untuned baseline. The code structure here establishes the end-to-end pipeline (data -> env -> train agent -> evaluate), which can be iteratively improved in later weeks.

Week 5: Enhancing Environment and Utility Functions
	•	Realism in Environment: Augment the CryptoPortfolioEnv with more realistic features based on findings during baseline testing. For instance, if not already done:
	•	Implement transaction costs thoroughly: deduct a fee (like 0.1%) of the traded value whenever the action changes the portfolio allocation. This encourages the agent to trade less frequently.
	•	Optionally, allow holding cash (if not already included) by having one dimension of the action vector represent cash (with price always 1). This requires the observation to include cash balance and the reward calculation to consider cash as yielding zero return.
	•	Ensure the action is properly normalized (the agent might output unconstrained numbers; map these to valid allocations, e.g., via a softmax or normalization step inside the environment).
	•	Metrics Module: Create utils/metrics.py with functions to compute evaluation metrics: e.g., Return on Investment (ROI), Sharpe ratio, Sortino ratio, Max Drawdown, Calmar ratio, etc. These metrics (also used in literature ￼) help to assess performance beyond raw returns. Use the portfolio value history from an episode to compute these.
	•	Update Evaluation: Expand evaluate_baseline.py to utilize the metrics module. After running the agent on test data, compute and print the metrics for the strategy. Also compute metrics for a benchmark strategy (like an equal-weight portfolio or buy-and-hold for comparison). This requires writing a small function for the benchmark strategy and feeding the same test data.
	•	Testing and Debugging: Verify that metrics calculations are correct (e.g., Sharpe ratio uses the proper risk-free rate assumption or period frequency). Test the environment with edge cases: e.g., extreme actions (all-in on one asset) to see if any divide-by-zero or constraint issues occur. By the end of Week 5, the environment and evaluation tools should be robust and feature-complete, ready to incorporate advanced modeling in subsequent weeks.

Week 6: Graph Construction and Feature Extraction
	•	Financial Graph Building: Implement graph/graph_builder.py to construct a graph of the crypto assets using historical data relationships. Use price series from the data pipeline to compute correlation or covariance between assets over a significant period. Create edges between assets that have high correlation or some domain-specific relationship (e.g., same sector/category if available). The graph can be represented with networkx, where nodes are assets and weighted edges represent relationship strength. We opt for a static graph structure based on long-term correlations or fundamental relationships, to avoid noise from constantly changing links ￼.
	•	Graph Feature Engineering: In graph/graph_features.py, develop methods to generate features from the graph for each asset. Two approaches to implement:
	•	Graph Statistics: Compute simple graph metrics per node (asset) such as degree, centrality, cluster membership, etc., and use them as additional features in the state. For example, an asset’s degree (number of strongly correlated neighbors) might indicate its connectivity in the market.
	•	Graph Neural Network Embeddings: Prepare to use a Graph Neural Network (e.g., GraphSAGE or GCN) to derive learned embeddings for each node. Using torch_geometric, define a graph data object with node initial features (could start with static features like volatility or market cap, or learned technical features). Implement a function that runs a forward pass of a GraphSAGE layer to produce an embedding for each asset ￼. Initially, you can train this GNN on historical data in an unsupervised way (or integrate training with the RL agent in Week 7).
	•	Graph Data Integration: Decide how to incorporate graph-based features into the RL environment. One plan: compute static node embeddings (or graph metrics) once and store them, then include each asset’s embedding in the observation vector. Alternatively, plan for dynamic computation where at each timestep a GNN could take current features to produce new embeddings (more complex). Document this decision in code comments and prepare the environment to accept graph inputs.
	•	Validation: Run graph_builder.py to generate the asset graph and visually inspect or log summary (e.g., print out top correlated asset pairs, graph connectivity). Ensure graph_features.py returns sensible outputs (e.g., embeddings have the expected dimension). By end of Week 6, the project will have a constructed graph of cryptos and a way to derive features from it, setting the stage for graph-guided RL.

Week 7: Integration of Graph Features into the RL Agent
	•	Custom Policy Network (Graph + RL): Develop a custom actor-critic network that can handle graph-structured data. In agents/graph_policy.py, define a PyTorch module that accepts observations including asset features and possibly the adjacency matrix. For example, implement a GraphSAGE-based feature extractor within the policy: use torch_geometric.nn.GraphSAGE layers (or manual message passing) to transform asset features using the graph adjacency ￼. The GraphSAGE can produce an embedding for each asset, which you then aggregate (e.g., via pooling or taking a weighted sum based on the current portfolio weights) to feed into the policy’s fully connected layers. This will allow the agent to understand relationships between assets when making decisions, aligning with research that GraphSAGE can significantly improve PPO performance in portfolio tasks ￼.
	•	Policy Integration with Stable-Baselines3: Modify the training script (or create a new one, agents/train_graph_agent.py) to use the custom graph-aware policy. Stable-Baselines3 allows custom policies by subclassing ActorCriticPolicy or using policy_kwargs. Integrate the GraphPolicy so that SB3’s PPO/SAC uses it for forward passes. Pass the pre-built graph (from Week 6) to the policy or store it inside the environment for access.
	•	State Augmentation: Adjust the CryptoPortfolioEnv observation space if needed to include any new graph-based features. For instance, if you decided to append static graph embeddings to each asset’s feature vector, ensure the observation reflects this (observation might become a larger vector per asset). The environment’s reset() and step() should retrieve or update graph-derived features (if dynamic) at each step.
	•	Training with Graph Features: Retrain the agent using train_graph_agent.py, observing if training is stable. This graph-guided agent should, in theory, learn improved strategies by exploiting inter-asset relations (e.g., hedging or co-movement patterns). Monitor training reward to see if it improves over the baseline.
	•	Preliminary Evaluation: After training, evaluate the new model (update the evaluation script to handle the new policy and possibly new observation structure). Compare the performance metrics to the baseline from Week 5. The coding focus this week is ensuring the integration works (no shape mismatches or runtime errors) and that the graph features are indeed being passed through. Debug by printing intermediate shapes or a few network outputs if necessary. By the end of Week 7, the RL agent code can leverage the graph of assets, fulfilling the “graph-guided” aspect of the project.

Week 8: Incorporating HMM/Kalman for Market Regime Adaptation
	•	Regime Detection with HMM: Implement models/regime_hmm.py to capture market regimes. Using hmmlearn, train a Hidden Markov Model on a relevant time series (e.g., the overall portfolio return or a market index, or even each asset individually if needed) to identify discrete market regimes (bull vs bear, high-volatility vs low-volatility). Choose number of states (e.g., 2 or 3) and fit the HMM on historical training data returns. Once trained, use the HMM to label each time step in the data with a regime index (or probability distribution over regimes). This can be done offline for the whole dataset. Quickly verify that the HMM states correspond to intuitive regimes (e.g., one state might correspond to turbulent market periods).
	•	Kalman Filter for Adaptive Signals: Implement models/kalman_filter.py to utilize a Kalman Filter (via pykalman) for tracking or smoothing financial signals. For example, use a Kalman Filter to estimate the latent trend of each asset’s price or the market as a whole. The filter can take in price or return as observations and produce a smoothed estimate of true value or trend and volatility. Fit the filter parameters on training data. The result is that at each time step, the filter can provide an updated estimate (e.g., if prices jump, the Kalman filter smooths out noise and reveals underlying trend).
	•	Adaptive State Augmentation: Modify the environment to include adaptive features from these models in its observation. For instance:
	•	Add the current regime label (HMM state) as an input feature for the agent at each timestep. This could be one-hot encoded or as probabilities of each regime. This will allow the policy network to condition its actions on the market regime, enabling adaptation to new conditions ￼. We emphasize quick detection of regime shifts is crucial, as undetected shifts can lead to losses ￼, so the agent having this info should improve robustness.
	•	Include Kalman filter outputs as features, such as the filtered price trend or a volatility estimate. This gives the agent a sense of the “smoothed” market direction or risk level, complementing the raw price changes.
	•	Environment Updates: In CryptoPortfolioEnv.step(), after moving to the next time step, update the HMM regime state (e.g., look up the precomputed regime for the new date) and update the Kalman filter with the new observation to get the latest estimate. Include these values in the observation returned. Adjust the observation_space definition if new features are added.
	•	Agent Training with Adaptation: Retrain the RL agent (possibly create agents/train_final_agent.py) with the environment now providing graph + regime + Kalman features. The policy network may need minor adjustments to accept the larger input (for example, if the regime is one-hot, just appending to input vector). Train using PPO/SAC as before. The expectation is that the agent can now learn different behaviors in different regimes (since it knows the regime) and respond to regime changes more appropriately.
	•	Validation: After training, evaluate on the test set and check if performance metrics improved especially in volatile periods (e.g., Sharpe ratio might improve if the agent manages risk in high-volatility regimes). From a coding perspective, ensure the HMM and Kalman integrations do not introduce lookahead bias (the HMM should use past data up to t, not future, when labeling regime at time t – if precomputed offline, it should be based on past data or carefully not leaking future info). By the end of Week 8, the codebase will incorporate adaptive elements (regime-switching awareness and signal smoothing), rounding out the core features of the project.

Week 9: Final Model Tuning, Testing, and Evaluation
	•	Hyperparameter Configuration: Introduce a configuration file (YAML/JSON or Python dict) for hyperparameters to make experiments easily configurable without code changes. This can include RL learning rates, number of training timesteps, HMM number of states, etc. Adjust the training scripts to load these settings. Conduct multiple training runs with different configurations (e.g., compare PPO vs SAC, or different regime state counts) – code-wise, implement loops or shell scripts to run these variations and save results. (The focus is on coding the ability to do this systematically, rather than manual tuning.)
	•	Comprehensive Evaluation Script: Expand the evaluation module to handle multiple models and benchmarks. For example, create agents/compare_strategies.py that can load several saved models (baseline, graph-only, final adaptive model) and evaluate each on the same test data, outputting a comparison of performance metrics side by side. Include statistical tests or at least multiple runs for each to gauge stability (this could mean running each model for a few different random seeds and averaging results).
	•	Visualization: In utils/plots.py, add functions to plot results: e.g., portfolio value over time for each strategy, a bar chart of final ROI or Sharpe for different models, and maybe regime vs allocation if insightful. Generate these plots in the evaluation script to visually verify the adaptive agent’s behavior (for instance, see if it reduces exposure during bearish regimes).
	•	Code Testing & QA: By this stage, develop unit tests for critical functions if not already (using unittest or pytest). For example, test that the reward calculation is consistent with manual computations, test that HMM returns expected number of states, etc. Also perform integration tests: run a short training + evaluation pipeline automatically to ensure nothing breaks when all pieces are combined. Fix any bugs revealed (e.g., shape mismatches, off-by-one index errors at episode boundaries, etc.).
	•	Performance Optimizations: Profile the runtime of training and inference. If graph computations or HMM updates are slowing things down, consider caching results (e.g., precompute graph embeddings per time step if using dynamic features, or pre-label regimes so the environment just looks them up). Optimize data structures (use NumPy arrays instead of Python lists where applicable, etc.). Ensure that even with 10+ assets and thousands of timesteps, the training loop runs efficiently. By the end of Week 9, the code should be well-tested, and the final model’s performance clearly evaluated against baselines using the tools implemented.

Week 10: Code Refinement and Project Wrap-Up
	•	Refactoring and Cleanup: Go through the codebase to refactor and improve clarity and modularity. Remove redundant code, ensure functions are not overly long (break them into helper functions in utils if needed). For example, if the environment step has grown complex, refactor parts of it (like reward calculation, cost deduction) into separate functions within the class or a utility module. Make sure module responsibilities are clear (data pipeline vs environment vs agent training are cleanly separated).
	•	Documentation: Write or update docstrings for all classes, methods, and functions to clearly explain their purpose and usage. Ensure the README (or a separate documentation file) is updated to reflect how to run the code, what each module does, and how to reproduce the results. Although the focus is coding, in-code documentation is crucial for maintainability.
	•	Final Integration Script: Create a top-level script run_full_pipeline.py that ties everything together for reproducibility. This script can: load config, prepare data, instantiate and train the final agent, then evaluate it and output metrics/plots. The idea is to allow a single command to execute the entire experiment pipeline. This might involve calling functions from modules rather than duplicating code.
	•	Reproducibility Check: Do a clean run of the entire pipeline using the final script on a fresh environment (simulate by clearing any cached state). The result should be a trained model and evaluation output that matches expectations. Ensure random seeds are set in all places (numpy, torch, random) for determinism.
	•	Buffer & Contingency: Use this final week to address any remaining issues that surfaced in integration. For example, if the agent is still unstable, you might implement a last-minute fix like clipping extreme actions or rewards (code adjustments to stabilize training). If certain components are unused or proved ineffective, remove or flag them to avoid confusion.
	•	Deliverables: By the end of Week 10, the project should deliver a clean, modular codebase with all components (data pipeline, graph construction, HMM/Kalman integration, custom environment, RL training scripts, evaluation and plotting tools) implemented. All code will be organized and documented such that another developer or researcher can understand and run the adaptive graph-guided RL framework on crypto portfolio data. The focus on clean design (config files, utils, tests) ensures the project is reproducible and extensible, fulfilling the original goals of the 10-week project.